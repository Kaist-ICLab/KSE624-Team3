#!/usr/bin/env python
# coding: utf-8

'''
Final Project for KSE624 Mobile and Pervasive Computing for Knowledge Services Spring 2020 at KAIST

Last Updated Date: July 01 2020
Authors: 
    Rafikatiwi Nur Pujiarti
    Willmer R. Quinones

-----------------------------

clothes_recognition.py

(1) detectPerson: 
	(a) Given an image, returns the location of a person (if any)
	(b) Divide the person image into upper body and lower body
(2) detectClothe:
	(a) Use a pretrained deep learning model to classify and return the clothes of the
		user for both upper body (shirt, coat, etc) and lower body (shorts, pants..)
'''

## Necessary Packages
import numpy as np
from PIL import Image
import requests
import json
import torch
import torchvision.transforms as transforms

device = torch.device("cuda" if (torch.cuda.is_available()) else "cpu")

def detectPerson(img_path: str):

	'''

	J-Bot uses Microsoft Azure API to detect the user infront of the camera:
		https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/

	Please sign-up to Azure to get your own subscription key and endpoint url
	Please also follow the instruction from Microsoft webpage
		https://docs.microsoft.com/en-us/azure/cognitive-services/Computer-vision/quickstarts/python-disk

	-----------------------------

	This function returns the rectangle that is boxing the detected person on the image (if any)

	Args:
		- img_path: path of the image generated by J-Bot camera
	Returns:
		- x: x-coordinate of the top-left corner of the box
		- y: y-coordinate of the top-left corner of the box
		- w: Width of the box
		- h: Height of the box

	'''

    subscription_key = '' # <Your subscription key>
    analyze_url = '' # <your endpoint> + 'vision/v3.0/analyze'

    headers = {'Ocp-Apim-Subscription-Key': subscription_key,
            'Content-Type': 'application/octet-stream'}
    params = {'visualFeatures': 'Categories,Description,Objects'}
    
    with open(img_path, 'rb') as data:
        response = requests.post(
          analyze_url, headers=headers, params=params, data=data)

    response.raise_for_status()
    response = response.json()

    # If there are not response, or the API does not detect a person, return empty coordinates
    if len(response) == 0:
        return (0, 0, 0, 0)
    
    if 'objects' not in response.keys():
    	return (0, 0, 0, 0)

    person_detected = [d for d in response['objects'] if 'person' in d.values()]

    if len(person_detected) == 0:
        return (0, 0, 0, 0)
    
    # Get the location of the person within the image
    x, y, w, h = person_detected[0]['rectangle'].values()    

    return (x, y, w, h)

def detectClothes(img_path, clothe_model):  
  
	'''

	Classify the upper and lower clothes of the user given an image (from J-Bot camera).

	Our model was trained with a subset of Large-scale Fashion Database
		http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html

	Args:
		- img_path: path of the image generated by J-Bot camera
		- clothe_model: PyTorch model to classify the image
	Returns:
		- x: x-coordinate of the top-left corner of the box
		- y: y-coordinate of the top-left corner of the box
		- w: Width of the box
		- h: Height of the box

	'''

    ori_img =  np.array(Image.open(img_path))

    H, _, _ = ori_img.shape

    # Get the user image
    x, y, width, height = detectPerson(img_path)

    # If no one is detected, returns an empty string
    if width == 0:
        return ('', '')

    # Separating the upper body from the lower body
    # Upper-body: Upper half of the whole body
    # Lower-body: 75% of the body from bottom to top
    x1 = x
    x2 = x + width
    top_y1 = y
    top_y2 = int(y + 0.5 * height)
    bot_y1 = int(y + 0.25 * height)
    bot_y2 = height

    top_img = ori_img[top_y1:top_y2, x1:x2, :]
    bot_img = ori_img[bot_y1:bot_y2, x1:x2, :]

    # The classes are divided into top and bottom classes
    top_classes = ['shirt', 'thick clothes', 'thin jacket']
    bot_classes = ['long pants', 'shorts']

    # Our model was training for 100x100x3 images, but you can adjust
    trans_params = transforms.Compose([
                                    transforms.ToPILImage(),
                                    transforms.Resize((100, 100)),    
                                    transforms.Grayscale(num_output_channels = 3),
                                    transforms.ToTensor(),
                                    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    ])

    top_input = trans_params(top_img).to(device)
    bot_input = trans_params(bot_img).to(device)

    outputs = clothe_model(top_input.unsqueeze(0)).data[0]

    # Getting the prediction from the deep learning model
    top_preds = torch.cat([outputs[1].unsqueeze(0), outputs[3:]])
    _, top_pred = top_preds.data.max(0)
    top_class = top_classes[top_pred.item()]

    bot_preds = torch.cat([outputs[0].unsqueeze(0), outputs[2].unsqueeze(0)])
    _, bot_pred = bot_preds.data.max(0)
    bot_class = bot_classes[bot_pred.item()]

    return (top_class, bot_class)

